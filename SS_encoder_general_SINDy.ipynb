{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysindy as ps\n",
    "\n",
    "import deepSI\n",
    "from deepSI.fit_systems import SS_encoder_general\n",
    "from deepSI.fit_systems.encoders import default_encoder_net, default_state_net, default_output_net\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "\n",
    "import deepSI\n",
    "from deepSI import System_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r\"C:\\Users\\20173928\\OneDrive - TU Eindhoven\\Documents\\Master\\thesis\\mscth\\data\\own_data\"\n",
    "\n",
    "out = loadmat(os.path.join(save_dir,'MSD_x_data.mat'))\n",
    "x_data = out['x']\n",
    "\n",
    "out = loadmat(os.path.join(save_dir,'MSD_u_data.mat'))\n",
    "u_data = out['u']\n",
    "\n",
    "out = loadmat(os.path.join(save_dir,'MSD_y_data.mat'))\n",
    "y_data = out['y']\n",
    "\n",
    "train, test = System_data(u=u_data[:-1000,0],y=y_data[:-1000,0]), System_data(u=u_data[-1000:,0],y=y_data[-1000:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SS_encoder_general_eq(SS_encoder_general):\n",
    "    def __init__(self, nx=10, na=20, nb=20, feedthrough=False, \\\n",
    "        e_net=default_encoder_net, f_net=default_state_net, h_net=default_output_net, \\\n",
    "        e_net_kwargs={},           f_net_kwargs={},         h_net_kwargs={}, na_right=0, nb_right=0, \\\n",
    "        gamma=1e-4):\n",
    "\n",
    "        super(SS_encoder_general_eq, self).__init__()\n",
    "        self.nx, self.na, self.nb = nx, na, nb\n",
    "        self.k0 = max(self.na,self.nb)\n",
    "        \n",
    "        self.e_net = e_net\n",
    "        self.e_net_kwargs = e_net_kwargs\n",
    "\n",
    "        self.f_net = f_net\n",
    "        self.f_net_kwargs = f_net_kwargs\n",
    "\n",
    "        self.h_net = h_net\n",
    "        self.h_net_kwargs = h_net_kwargs\n",
    "\n",
    "        self.feedthrough = feedthrough\n",
    "        self.na_right = na_right\n",
    "        self.nb_right = nb_right\n",
    "        ######################################\n",
    "        # args added for feature transform and\n",
    "        # regurlarization\n",
    "        self.gamma = gamma\n",
    "        ######################################\n",
    "\n",
    "    def init_nets(self, nu, ny): # a bit weird\n",
    "        na_right = self.na_right if hasattr(self,'na_right') else 0\n",
    "        nb_right = self.nb_right if hasattr(self,'nb_right') else 0\n",
    "        self.encoder = self.e_net(nb=(self.nb+nb_right), nu=nu, na=(self.na+na_right), ny=ny, nx=self.nx, **self.e_net_kwargs)\n",
    "        ######################################\n",
    "        ###### change fn intialization #######\n",
    "        self.fn     =      self.f_net(nx=self.nx, nu=nu, **self.f_net_kwargs)\n",
    "        ######################################\n",
    "        if self.feedthrough:\n",
    "            self.hn =      self.h_net(nx=self.nx, ny=ny, nu=nu,                     **self.h_net_kwargs) \n",
    "        else:\n",
    "            self.hn =      self.h_net(nx=self.nx, ny=ny,                            **self.h_net_kwargs) \n",
    "\n",
    "    def loss(self, uhist, yhist, ufuture, yfuture, loss_nf_cutoff=None, **Loss_kwargs):\n",
    "        x = self.encoder(uhist, yhist) #initialize Nbatch number of states\n",
    "        print(torch.max(x))\n",
    "        print()\n",
    "        errors = []\n",
    "        for y, u in zip(torch.transpose(yfuture,0,1), torch.transpose(ufuture,0,1)): #iterate over time\n",
    "            error = nn.functional.mse_loss(y, self.hn(x,u) if self.feedthrough else self.hn(x))\n",
    "            ##################################\n",
    "            ## add penalty to weights in fn ##\n",
    "            params = [*self.fn.parameters()]\n",
    "            weights = [x.view(-1) for x in params][0]\n",
    "            error += self.gamma*torch.norm(weights, 1)\n",
    "            ##################################\n",
    "            errors.append(error) #calculate error after taking n-steps\n",
    "            if loss_nf_cutoff is not None and error.item()>loss_nf_cutoff:\n",
    "                print(len(errors), end=' ')\n",
    "                break\n",
    "            x = self.fn(x,u) #advance state. \n",
    "            \n",
    "        return torch.mean(torch.stack(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input[:,-1]\n",
    "    \n",
    "class simple_Linear(torch.nn.Module):\n",
    "    def __init__(self, nx, nu, **kwargs):\n",
    "        super(simple_Linear, self).__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.nu = kwargs['u']\n",
    "\n",
    "        self.feature_library = kwargs['feature_library']\n",
    "        test_sample = torch.rand(1,self.nx+self.nu, requires_grad=True)\n",
    "        self.nf = (self.feature_library.fit_transform(test_sample)).shape[1]\n",
    "        \n",
    "        self.layer = nn.Linear(self.nf, nx, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = torch.hstack((x, u.unsqueeze(1)))\n",
    "        Theta = self.feature_library.fit_transform(x)\n",
    "        out = self.layer(Theta)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_library():\n",
    "    def __init__(\n",
    "            self,\n",
    "            functions,\n",
    "            interaction_only=True\n",
    "    ):\n",
    "        self.functions = functions\n",
    "        self.interaction_only = interaction_only\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # off set\n",
    "        out_feature = ((X[:,0])**0).unsqueeze(1)\n",
    "        if self.interaction_only:\n",
    "            for f in self.functions:\n",
    "                out_feature = torch.hstack((out_feature, f(X)))\n",
    "            return out_feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return x\n",
    "\n",
    "def f2(x):\n",
    "  return x**2\n",
    "\n",
    "def sin(x):\n",
    "  return np.sin(x)\n",
    "\n",
    "def f3(x):\n",
    "  return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions = [lambda x:x,\n",
    "#              lambda x:x**2,\n",
    "#              lambda x:x**3]\n",
    "functions = [f, f3]\n",
    "\n",
    "poly = feature_library(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilizing the model and optimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training array =  1.1 GB!\n",
      "Consider using online_construct=True (in loss_kwargs) or let make_training_data return a Dataset to reduce data-usage\n",
      "N_training_samples = 498851, batch_size = 2, N_batch_updates_per_epoch = 249425\n",
      "Initial Validation sim-NRMS= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2537, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early due to a KeyboardInterrupt\n",
      "no best checkpoint found keeping last\n",
      "Loaded model with best known validation sim-NRMS of    inf which happened on epoch 0 (epoch_id=0.00)\n"
     ]
    }
   ],
   "source": [
    "# fit_sys = deepSI.fit_systems.SS_encoder_general(nx=2, na=50, nb=50)\n",
    "\n",
    "nx, nu = 2, 1 # state dimension and inputs\n",
    "na, nb = 5, 5\n",
    "\n",
    "f_net = simple_Linear\n",
    "f_net_kwargs= f_net_kwargs={\"feature_library\": poly, \"u\": nu, \"nf\": 10}\n",
    "\n",
    "h_net = identity\n",
    "h_net_kwargs = {}\n",
    "\n",
    "fit_sys = SS_encoder_general_eq(nx=2, na=50, nb=50, \\\n",
    "                                f_net=f_net, f_net_kwargs=f_net_kwargs,\\\n",
    "                                h_net=identity)\n",
    "\n",
    "# train, test = deepSI.datasets.Silverbox()\n",
    "# train, test = train[:1000], test[:1000]\n",
    "\n",
    "fit_sys.fit(train, test, epochs=1, batch_size = 2, optimizer_kwargs={\"lr\": 1e-2}, loss_kwargs=dict(nf=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Parameter containing:\\ntensor([[-0.1435, -0.3232,  0.3120,  0.3655,  0.0963, -0.2028,  0.1711],\\n        [ 0.3076,  0.3702,  0.1113, -0.1160, -0.0018, -0.0215,  0.1180]],\\n       requires_grad=True)]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str([*fit_sys.fn.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
