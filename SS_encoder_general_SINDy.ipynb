{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysindy as ps\n",
    "\n",
    "import deepSI\n",
    "from deepSI.fit_systems import SS_encoder_general\n",
    "from deepSI.fit_systems.encoders import default_encoder_net, default_state_net, default_output_net\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SS_encoder_general_eq(SS_encoder_general):\n",
    "    def __init__(self, nx=10, na=20, nb=20, feedthrough=False, \\\n",
    "        e_net=default_encoder_net, f_net=default_state_net, h_net=default_output_net, \\\n",
    "        e_net_kwargs={},           f_net_kwargs={},         h_net_kwargs={}, na_right=0, nb_right=0, \\\n",
    "        gamma=1e-4):\n",
    "\n",
    "        super(SS_encoder_general_eq, self).__init__()\n",
    "        self.nx, self.na, self.nb = nx, na, nb\n",
    "        self.k0 = max(self.na,self.nb)\n",
    "        \n",
    "        self.e_net = e_net\n",
    "        self.e_net_kwargs = e_net_kwargs\n",
    "\n",
    "        self.f_net = f_net\n",
    "        self.f_net_kwargs = f_net_kwargs\n",
    "\n",
    "        self.h_net = h_net\n",
    "        self.h_net_kwargs = h_net_kwargs\n",
    "\n",
    "        self.feedthrough = feedthrough\n",
    "        self.na_right = na_right\n",
    "        self.nb_right = nb_right\n",
    "        ######################################\n",
    "        # args added for feature transform and\n",
    "        # regurlarization\n",
    "        self.gamma = gamma\n",
    "        ######################################\n",
    "\n",
    "    def init_nets(self, nu, ny): # a bit weird\n",
    "        na_right = self.na_right if hasattr(self,'na_right') else 0\n",
    "        nb_right = self.nb_right if hasattr(self,'nb_right') else 0\n",
    "        self.encoder = self.e_net(nb=(self.nb+nb_right), nu=nu, na=(self.na+na_right), ny=ny, nx=self.nx, **self.e_net_kwargs)\n",
    "        ######################################\n",
    "        ###### change fn intialization #######\n",
    "        self.fn     =      self.f_net(nx=self.nx, nu=nu, **self.f_net_kwargs)\n",
    "        ######################################\n",
    "        if self.feedthrough:\n",
    "            self.hn =      self.h_net(nx=self.nx, ny=ny, nu=nu,                     **self.h_net_kwargs) \n",
    "        else:\n",
    "            self.hn =      self.h_net(nx=self.nx, ny=ny,                            **self.h_net_kwargs) \n",
    "\n",
    "    def loss(self, uhist, yhist, ufuture, yfuture, loss_nf_cutoff=None, **Loss_kwargs):\n",
    "        x = self.encoder(uhist, yhist) #initialize Nbatch number of states\n",
    "        errors = []\n",
    "        \n",
    "        for y, u in zip(torch.transpose(yfuture,0,1), torch.transpose(ufuture,0,1)): #iterate over time\n",
    "            error = nn.functional.mse_loss(y, self.hn(x,u) if self.feedthrough else self.hn(x))\n",
    "            ##################################\n",
    "            ## add penalty to weights in fn ##\n",
    "            params = [*self.fn.parameters()]\n",
    "            weights = [x.view(-1) for x in params][0]\n",
    "            error += self.gamma*torch.norm(weights, 1)\n",
    "            ##################################\n",
    "            errors.append(error) #calculate error after taking n-steps\n",
    "            if loss_nf_cutoff is not None and error.item()>loss_nf_cutoff:\n",
    "                print(len(errors), end=' ')\n",
    "                break\n",
    "            x = self.fn(x,u) #advance state. \n",
    "            \n",
    "        return torch.mean(torch.stack(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input[:,-1]\n",
    "    \n",
    "class simple_Linear(torch.nn.Module):\n",
    "    def __init__(self, nx, nu, **kwargs):\n",
    "        super(simple_Linear, self).__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.nu = kwargs['u']\n",
    "\n",
    "        self.feature_library = kwargs['feature_library']\n",
    "        test_sample = torch.rand(1,self.nx+self.nu, requires_grad=True)\n",
    "        self.nf = (self.feature_library.fit_transform(test_sample)).shape[1]\n",
    "        \n",
    "        self.layer = nn.Linear(self.nf, nx, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = torch.hstack((x, u.unsqueeze(1)))\n",
    "        Theta = self.feature_library.fit_transform(x)\n",
    "        mu = torch.mean(Theta)\n",
    "        std = torch.std(Theta)\n",
    "        Theta = (Theta-mu)/std\n",
    "        out = self.layer(Theta)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_library():\n",
    "    def __init__(\n",
    "            self,\n",
    "            functions,\n",
    "            interaction_only=True\n",
    "    ):\n",
    "        self.functions = functions\n",
    "        self.interaction_only = interaction_only\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # off set\n",
    "        out_feature = ((X[:,0])**0).unsqueeze(1)\n",
    "        if self.interaction_only:\n",
    "            for f in self.functions:\n",
    "                out_feature = torch.hstack((out_feature, f(X)))\n",
    "            return out_feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return x\n",
    "\n",
    "def f2(x):\n",
    "  return x**2\n",
    "\n",
    "def f3(x):\n",
    "  return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions = [lambda x:x,\n",
    "#              lambda x:x**2,\n",
    "#              lambda x:x**3]\n",
    "functions = [f, f2, f3]\n",
    "\n",
    "poly = feature_library(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilizing the model and optimizer\n",
      "Size of the training array =  1.9 MB\n",
      "N_training_samples = 851, batch_size = 2, N_batch_updates_per_epoch = 425\n",
      "Initial Validation sim-NRMS= 1.0233330686621502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:38<00:38, 38.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## New lowest validation loss achieved ########### sim-NRMS = 1.0232632284580911\n",
      "Epoch    1, sqrt loss   1.052, Val sim-NRMS  1.023, Time Loss: 99.1%, data: 0.3%, val: 0.6%, 11.1 batches/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:16<00:00, 38.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## New lowest validation loss achieved ########### sim-NRMS = 1.0231937520263894\n",
      "Epoch    2, sqrt loss   1.051, Val sim-NRMS  1.023, Time Loss: 99.1%, data: 0.2%, val: 0.6%, 11.2 batches/sec\n",
      "Loaded model with best known validation sim-NRMS of  1.023 which happened on epoch 2 (epoch_id=2.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fit_sys = deepSI.fit_systems.SS_encoder_general(nx=2, na=50, nb=50)\n",
    "\n",
    "nx, nu = 2, 1 # state dimension and inputs\n",
    "na, nb = 5, 5\n",
    "\n",
    "f_net = simple_Linear\n",
    "f_net_kwargs= f_net_kwargs={\"feature_library\": poly, \"u\": nu, \"nf\": 10}\n",
    "\n",
    "h_net = identity\n",
    "h_net_kwargs = {}\n",
    "\n",
    "fit_sys = SS_encoder_general_eq(nx=2, na=50, nb=50, \\\n",
    "                                f_net=f_net, f_net_kwargs=f_net_kwargs,\\\n",
    "                                h_net=identity)\n",
    "\n",
    "train, test = deepSI.datasets.Silverbox()\n",
    "train, test = train[:1000], test[:1000]\n",
    "\n",
    "fit_sys.fit(train, test, epochs=20, batch_size = 2, optimizer_kwargs={\"lr\": 1e-7}, loss_kwargs=dict(nf=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0614,  0.1140, -0.0476,  0.1463,  0.1167,  0.1051,  0.1902,  0.2439,\n",
       "           0.1115,  0.0896],\n",
       "         [-0.1570,  0.2194,  0.0225,  0.1736,  0.2099, -0.2865, -0.2864,  0.1420,\n",
       "          -0.1513, -0.2849]], requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*fit_sys.fn.parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
