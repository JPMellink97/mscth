{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pysindy as ps\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from pysindy.utils import linear_damped_SHO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator_keywords = {}\n",
    "integrator_keywords['rtol'] = 1e-12\n",
    "integrator_keywords['method'] = 'LSODA'\n",
    "integrator_keywords['atol'] = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "dt = 0.01\n",
    "t_train = np.arange(0, 25, dt)\n",
    "t_train_span = (t_train[0], t_train[-1])\n",
    "x0_train = [2, 0]\n",
    "x_train = solve_ivp(linear_damped_SHO, t_train_span,\n",
    "                    x0_train, t_eval=t_train, **integrator_keywords).y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate\n",
    "def gen_diff(x_data, t_data, order=2, diff=None):\n",
    "    diff = ps.differentiation.FiniteDifference(order=order) if diff is None else diff\n",
    "    x_dot = diff._differentiate(x_data, t_data)\n",
    "    return x_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Theta(data, lib=\"poly\", order=3):\n",
    "    if isinstance(lib, str):\n",
    "        if lib==\"poly\":\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "            return PolynomialFeatures(order).fit_transform(data)[:,1:]\n",
    "        elif lib==\"trig\":\n",
    "            from pysindy.feature_library import FourierLibrary\n",
    "\n",
    "            return np.array(FourierLibrary(order).fit(data).transform(data))\n",
    "    else:\n",
    "        from pysindy.feature_library import CustomLibrary\n",
    "\n",
    "        return np.array(CustomLibrary(library_functions=lib).fit(data).transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_regression(theta: np.ndarray, uprime: np.ndarray,\n",
    "                         threshold: float, max_iterations: int) -> np.ndarray:\n",
    "    \"\"\"Finds a xi matrix that fits theta * xi = uprime, using the sequential\n",
    "    thresholded least-squares algorithm, which is a regression algorithm that\n",
    "    promotes sparsity.\n",
    "\n",
    "    The authors of the SINDy paper designed this algorithm as an alternative\n",
    "    to LASSO, because they found LASSO to be algorithmically unstable, and\n",
    "    computationally expensive for very large data sets.\n",
    "    \"\"\"\n",
    "    # Solve theta * xi = uprime in the least-squares sense.\n",
    "    xi = np.linalg.lstsq(theta, uprime, rcond=None)[0]\n",
    "    n = xi.shape[1]\n",
    "\n",
    "    print(xi)\n",
    "\n",
    "    # Add sparsity.\n",
    "    for i in range(max_iterations):\n",
    "        small_indices = np.abs(xi) < threshold\n",
    "        xi[small_indices] = 0\n",
    "        for j in range(n):\n",
    "            big_indices = np.logical_not(small_indices[:, j])\n",
    "            xi[big_indices, j] = np.linalg.lstsq(theta[:, big_indices],\n",
    "                                                 uprime[:, j],\n",
    "                                                 rcond=None)[0]\n",
    "\n",
    "    return xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10000129 -1.99999973]\n",
      " [ 1.99999972 -0.10000131]]\n"
     ]
    }
   ],
   "source": [
    "x_dot_train = gen_diff(x_train, t_train, order=3)\n",
    "Theta = gen_Theta(x_train, order=1)\n",
    "xi = calculate_regression(Theta, x_dot_train, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.        ,  0.        ],\n",
       "       [ 1.99760141, -0.03995736],\n",
       "       [ 1.99440741, -0.07981887],\n",
       "       ...,\n",
       "       [ 0.15601786,  0.0526537 ],\n",
       "       [ 0.1568827 ,  0.04947352],\n",
       "       [ 0.15768297,  0.04627988]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi\n",
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.8563318252563477\n",
      "Epoch [10/100], Loss: 0.7456226944923401\n",
      "Epoch [20/100], Loss: 0.6415801048278809\n",
      "Epoch [30/100], Loss: 0.5457475185394287\n",
      "Epoch [40/100], Loss: 0.4664328694343567\n",
      "Epoch [50/100], Loss: 0.4005952477455139\n",
      "Epoch [60/100], Loss: 0.34639623761177063\n",
      "Epoch [70/100], Loss: 0.32851308584213257\n",
      "Epoch [80/100], Loss: 0.31904423236846924\n",
      "Epoch [90/100], Loss: 0.3096032738685608\n",
      "Coefficients:\n",
      "[[ 2.4667026e-01 -8.1781449e-04]\n",
      " [-1.8657017e-03  1.0006127e+00]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SINDy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sparsity):\n",
    "        super(SINDy, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sparse_loss = nn.L1Loss()\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def lasso_loss(self, parameters):\n",
    "        l1_penalty = torch.norm(parameters, 1)\n",
    "        return self.sparsity * l1_penalty\n",
    "    \n",
    "    def fit(self, X, y, lr=0.01, epochs=100):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(X)\n",
    "            loss = self.sparse_loss(output, y) + self.lasso_loss(self.linear.weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Example usage\n",
    "# Assuming X is your input data (features) and y is your target data\n",
    "X = torch.tensor(x_train, dtype=torch.float32)\n",
    "y = torch.tensor(x_train, dtype=torch.float32)\n",
    "\n",
    "input_dim = X.shape[1]  # Number of input features\n",
    "output_dim = y.shape[1]  # Number of output features\n",
    "sparsity = 0.1  # Sparsity parameter\n",
    "\n",
    "model = SINDy(input_dim, output_dim, sparsity)\n",
    "model.fit(X, y)\n",
    "\n",
    "coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2500x4 and 2500x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[197], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Maximum degree of the polynomial terms\u001b[39;00m\n\u001b[0;32m     54\u001b[0m sparsity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Sparsity parameter\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sindy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(coefficients)\n",
      "Cell \u001b[1;32mIn[197], line 39\u001b[0m, in \u001b[0;36mtrain_sindy\u001b[1;34m(X, y, degree, sparsity, lr, epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 39\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((output \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m lasso_loss(model\u001b[38;5;241m.\u001b[39mcoefficients, sparsity)\n\u001b[0;32m     41\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Joost\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joost\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[197], line 14\u001b[0m, in \u001b[0;36mSINDy.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2500x4 and 2500x2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SINDy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, library_functions):\n",
    "        super(SINDy, self).__init__()\n",
    "        self.library_functions = library_functions\n",
    "        self.output_dim = output_dim\n",
    "        self.coefficients = nn.Parameter(torch.randn(len(library_functions), output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.coefficients)\n",
    "\n",
    "def generate_library_functions(X, degree):\n",
    "    # Generate library functions up to a certain degree\n",
    "    library_functions = []\n",
    "    for d in range(1, degree + 1):\n",
    "        for i in range(X.shape[1]):\n",
    "            library_functions.append(np.power(X[:, i], d))\n",
    "    library_functions = np.column_stack(library_functions)\n",
    "    return library_functions\n",
    "\n",
    "def lasso_loss(parameters, sparsity):\n",
    "    l1_penalty = torch.norm(parameters, 1)\n",
    "    return sparsity * l1_penalty\n",
    "\n",
    "def train_sindy(X, y, degree, sparsity, lr=0.01, epochs=100):\n",
    "    library_functions = generate_library_functions(X, degree)\n",
    "    input_dim = library_functions.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "\n",
    "    model = SINDy(input_dim, output_dim, library_functions)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.tensor(library_functions, dtype=torch.float32))\n",
    "        loss = torch.mean((output - torch.tensor(y, dtype=torch.float32)) ** 2) + lasso_loss(model.coefficients, sparsity)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    return model.coefficients.detach().numpy()\n",
    "\n",
    "# Example usage\n",
    "# Assuming X is your input data (features) and y is your target data\n",
    "X = np.array(x_train)\n",
    "y = np.array(x_train)\n",
    "\n",
    "degree = 2  # Maximum degree of the polynomial terms\n",
    "sparsity = 0.1  # Sparsity parameter\n",
    "coefficients = train_sindy(X, y, degree, sparsity)\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
