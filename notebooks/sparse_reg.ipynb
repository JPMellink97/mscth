{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pysindy as ps\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from pysindy.utils import linear_damped_SHO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import deepSI\n",
    "from deepSI import System_data\n",
    "\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator_keywords = {}\n",
    "integrator_keywords['rtol'] = 1e-12\n",
    "integrator_keywords['method'] = 'LSODA'\n",
    "integrator_keywords['atol'] = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "dt = 0.01\n",
    "t_train = np.arange(0, 25, dt)\n",
    "t_train_span = (t_train[0], t_train[-1])\n",
    "x0_train = [2, 0]\n",
    "x_train = solve_ivp(linear_damped_SHO, t_train_span,\n",
    "                    x0_train, t_eval=t_train, **integrator_keywords).y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate\n",
    "def gen_diff(x_data, t_data, order=2, diff=None):\n",
    "    diff = ps.differentiation.FiniteDifference(order=order) if diff is None else diff\n",
    "    x_dot = diff._differentiate(x_data, t_data)\n",
    "    return x_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Theta(data, lib=\"poly\", order=3):\n",
    "    if isinstance(lib, str):\n",
    "        if lib==\"poly\":\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "            return PolynomialFeatures(order).fit_transform(data)\n",
    "        elif lib==\"trig\":\n",
    "            from pysindy.feature_library import FourierLibrary\n",
    "\n",
    "            return np.array(FourierLibrary(order).fit(data).transform(data))\n",
    "    else:\n",
    "        from pysindy.feature_library import CustomLibrary\n",
    "\n",
    "        return np.array(CustomLibrary(library_functions=lib).fit(data).transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_regression(theta, x_dot, threshold, max_iterations):\n",
    "    # Solve theta * xi = x_dot in the least-squares sense.\n",
    "    xi = np.linalg.lstsq(theta, x_dot, rcond=None)[0]\n",
    "    n = xi.shape[1]\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        small_indices = np.abs(xi) < threshold\n",
    "        xi[small_indices] = 0\n",
    "        for j in range(n):\n",
    "            big_indices = np.logical_not(small_indices[:, j])\n",
    "            xi[big_indices, j] = np.linalg.lstsq(theta[:, big_indices],\n",
    "                                                 x_dot[:, j],\n",
    "                                                 rcond=None)[0]\n",
    "\n",
    "    return xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dot_train = gen_diff(x_train, t_train, order=3)\n",
    "Theta = gen_Theta(x_train, order=1)\n",
    "xi = calculate_regression(Theta, x_dot_train, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.54266415,  -1.6964389 ],\n",
       "       [-83.30041189, 277.65992274],\n",
       "       [-33.88886531, -21.90275012]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\20173928\\OneDrive - TU Eindhoven\\Documents\\Master\\thesis\\mscth\\data\\\\\"\n",
    "\n",
    "WIENER = \"WienerHammerBenchmark\"\n",
    "SILVER = \"SNLS80mV\"\n",
    "# SILVER = \"Schroeder80mV\"\n",
    "# change data set\n",
    "DATA = SILVER\n",
    "CSV = \".csv\"\n",
    "\n",
    "PATH = os.path.join(DATA_PATH, DATA+CSV)\n",
    "\n",
    "# load data\n",
    "data = genfromtxt(PATH, delimiter=\",\")\n",
    "data = data[1:,:-1] # snl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = data[:,0]\n",
    "V2 = data[:,1]\n",
    "\n",
    "N = V1.shape[0]\n",
    "fs = 610.35\n",
    "dt = 1/fs\n",
    "t = np.linspace(0,dt*N,N)\n",
    "\n",
    "silver_data = System_data(u=V1,y=V2)\n",
    "\n",
    "train, test = silver_data[40000:], silver_data[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.c_[train.u, train.y]\n",
    "t_train = np.linspace(0,dt*train.y.shape[0],train.y.shape[0])\n",
    "\n",
    "x_dot_train = gen_diff(x_train, t_train, order=3)\n",
    "Theta = gen_Theta(x_train, order=3)\n",
    "xi = calculate_regression(Theta, x_dot_train, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.25619432e-01, -1.65198612e+00],\n",
       "       [-8.22431281e+01,  2.73479044e+02],\n",
       "       [-3.20677940e+01, -2.12289132e+01],\n",
       "       [-2.16807497e+01, -9.99347134e+01],\n",
       "       [ 8.85763345e+00, -1.41088658e+01],\n",
       "       [ 8.96973000e+00,  1.85929420e+00],\n",
       "       [ 8.46868965e+02,  2.88229138e+03],\n",
       "       [-5.56726330e+02,  1.38226434e+03],\n",
       "       [-6.74886684e+02,  8.44847238e+01],\n",
       "       [-1.60031315e+02, -1.52824848e+02]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/20000], Loss: 28.63304901123047\n",
      "Epoch [200/20000], Loss: 27.174068450927734\n",
      "Epoch [300/20000], Loss: 25.773475646972656\n",
      "Epoch [400/20000], Loss: 24.427539825439453\n",
      "Epoch [500/20000], Loss: 23.134105682373047\n",
      "Epoch [600/20000], Loss: 21.891368865966797\n",
      "Epoch [700/20000], Loss: 20.69769287109375\n",
      "Epoch [800/20000], Loss: 19.551549911499023\n",
      "Epoch [900/20000], Loss: 18.451499938964844\n",
      "Epoch [1000/20000], Loss: 17.396162033081055\n",
      "Epoch [1100/20000], Loss: 16.384220123291016\n",
      "Epoch [1200/20000], Loss: 15.414421081542969\n",
      "Epoch [1300/20000], Loss: 14.485552787780762\n",
      "Epoch [1400/20000], Loss: 13.596447944641113\n",
      "Epoch [1500/20000], Loss: 12.745987892150879\n",
      "Epoch [1600/20000], Loss: 11.933093070983887\n",
      "Epoch [1700/20000], Loss: 11.156712532043457\n",
      "Epoch [1800/20000], Loss: 10.415834426879883\n",
      "Epoch [1900/20000], Loss: 9.709476470947266\n",
      "Epoch [2000/20000], Loss: 9.0366792678833\n",
      "Epoch [2100/20000], Loss: 8.396519660949707\n",
      "Epoch [2200/20000], Loss: 7.788087844848633\n",
      "Epoch [2300/20000], Loss: 7.210491180419922\n",
      "Epoch [2400/20000], Loss: 6.662866592407227\n",
      "Epoch [2500/20000], Loss: 6.144360542297363\n",
      "Epoch [2600/20000], Loss: 5.65413236618042\n",
      "Epoch [2700/20000], Loss: 5.191350936889648\n",
      "Epoch [2800/20000], Loss: 4.75520133972168\n",
      "Epoch [2900/20000], Loss: 4.344870090484619\n",
      "Epoch [3000/20000], Loss: 3.9595541954040527\n",
      "Epoch [3100/20000], Loss: 3.598450183868408\n",
      "Epoch [3200/20000], Loss: 3.260761022567749\n",
      "Epoch [3300/20000], Loss: 2.9456872940063477\n",
      "Epoch [3400/20000], Loss: 2.6524312496185303\n",
      "Epoch [3500/20000], Loss: 2.3801915645599365\n",
      "Epoch [3600/20000], Loss: 2.1281609535217285\n",
      "Epoch [3700/20000], Loss: 1.895537257194519\n",
      "Epoch [3800/20000], Loss: 1.6815060377120972\n",
      "Epoch [3900/20000], Loss: 1.4852509498596191\n",
      "Epoch [4000/20000], Loss: 1.3059496879577637\n",
      "Epoch [4100/20000], Loss: 1.1427747011184692\n",
      "Epoch [4200/20000], Loss: 0.9948958158493042\n",
      "Epoch [4300/20000], Loss: 0.8614782691001892\n",
      "Epoch [4400/20000], Loss: 0.741677463054657\n",
      "Epoch [4500/20000], Loss: 0.6346568465232849\n",
      "Epoch [4600/20000], Loss: 0.5395777225494385\n",
      "Epoch [4700/20000], Loss: 0.45560213923454285\n",
      "Epoch [4800/20000], Loss: 0.38190343976020813\n",
      "Epoch [4900/20000], Loss: 0.3176526427268982\n",
      "Epoch [5000/20000], Loss: 0.2620496153831482\n",
      "Epoch [5100/20000], Loss: 0.21430087089538574\n",
      "Epoch [5200/20000], Loss: 0.17363575100898743\n",
      "Epoch [5300/20000], Loss: 0.13931018114089966\n",
      "Epoch [5400/20000], Loss: 0.11060943454504013\n",
      "Epoch [5500/20000], Loss: 0.0868539810180664\n",
      "Epoch [5600/20000], Loss: 0.06740362197160721\n",
      "Epoch [5700/20000], Loss: 0.05166175961494446\n",
      "Epoch [5800/20000], Loss: 0.03907784819602966\n",
      "Epoch [5900/20000], Loss: 0.02914905920624733\n",
      "Epoch [6000/20000], Loss: 0.02142305113375187\n",
      "Epoch [6100/20000], Loss: 0.015500578097999096\n",
      "Epoch [6200/20000], Loss: 0.011031114496290684\n",
      "Epoch [6300/20000], Loss: 0.007714362815022469\n",
      "Epoch [6400/20000], Loss: 0.0052960701286792755\n",
      "Epoch [6500/20000], Loss: 0.003565795486792922\n",
      "Epoch [6600/20000], Loss: 0.002351987175643444\n",
      "Epoch [6700/20000], Loss: 0.0015179126057773829\n",
      "Epoch [6800/20000], Loss: 0.000957494368776679\n",
      "Epoch [6900/20000], Loss: 0.0005895028007216752\n",
      "Epoch [7000/20000], Loss: 0.000354034302290529\n",
      "Epoch [7100/20000], Loss: 0.00020674247934948653\n",
      "Epoch [7200/20000], Loss: 0.00011742685455828905\n",
      "Epoch [7300/20000], Loss: 6.472787208622321e-05\n",
      "Epoch [7400/20000], Loss: 3.4557906474219635e-05\n",
      "Epoch [7500/20000], Loss: 1.7929740351974033e-05\n",
      "Epoch [7600/20000], Loss: 8.910083124646917e-06\n",
      "Epoch [7700/20000], Loss: 4.348780748841818e-06\n",
      "Epoch [7800/20000], Loss: 1.975799932552036e-06\n",
      "Epoch [7900/20000], Loss: 9.471730209043017e-07\n",
      "Epoch [8000/20000], Loss: 3.7453841628121154e-07\n",
      "Epoch [8100/20000], Loss: 1.5119361762572225e-07\n",
      "Epoch [8200/20000], Loss: 5.97281086811563e-08\n",
      "Epoch [8300/20000], Loss: 2.1268743211066976e-08\n",
      "Epoch [8400/20000], Loss: 2.944068633325969e-08\n",
      "Epoch [8500/20000], Loss: 2.1211707945667513e-09\n",
      "Epoch [8600/20000], Loss: 1.975786645402877e-09\n",
      "Epoch [8700/20000], Loss: 1.9766839276513792e-09\n",
      "Epoch [8800/20000], Loss: 1.8841381788092804e-09\n",
      "Epoch [8900/20000], Loss: 1.5462319202796948e-09\n",
      "Epoch [9000/20000], Loss: 1.4483396704179086e-09\n",
      "Epoch [9100/20000], Loss: 1.3266678866585835e-09\n",
      "Epoch [9200/20000], Loss: 3.515764390726872e-08\n",
      "Epoch [9300/20000], Loss: 1.0362251057216554e-09\n",
      "Epoch [9400/20000], Loss: 9.66180135897332e-10\n",
      "Epoch [9500/20000], Loss: 8.668284978696761e-10\n",
      "Epoch [9600/20000], Loss: 2.8254105721714495e-09\n",
      "Epoch [9700/20000], Loss: 6.668403607079654e-10\n",
      "Epoch [9800/20000], Loss: 6.380331263322603e-10\n",
      "Epoch [9900/20000], Loss: 5.84468362596624e-10\n",
      "Epoch [10000/20000], Loss: 5.339585440466976e-10\n",
      "Epoch [10100/20000], Loss: 4.902095951386798e-10\n",
      "Epoch [10200/20000], Loss: 4.3422962447969837e-10\n",
      "Epoch [10300/20000], Loss: 3.3739086524597894e-10\n",
      "Epoch [10400/20000], Loss: 3.3732680537745807e-10\n",
      "Epoch [10500/20000], Loss: 2.0696704439160385e-07\n",
      "Epoch [10600/20000], Loss: 1.758479611924102e-10\n",
      "Epoch [10700/20000], Loss: 1.693134937807983e-10\n",
      "Epoch [10800/20000], Loss: 1.6918040579572136e-10\n",
      "Epoch [10900/20000], Loss: 1.692076340154003e-10\n",
      "Epoch [11000/20000], Loss: 1.6918558221057367e-10\n",
      "Epoch [11100/20000], Loss: 1.693809814629077e-10\n",
      "Epoch [11200/20000], Loss: 1.6928457569065358e-07\n",
      "Epoch [11300/20000], Loss: 4.8150261555690577e-11\n",
      "Epoch [11400/20000], Loss: 3.054081640363471e-11\n",
      "Epoch [11500/20000], Loss: 3.0582241600241034e-11\n",
      "Epoch [11600/20000], Loss: 3.084723795843125e-11\n",
      "Epoch [11700/20000], Loss: 3.063577863615663e-11\n",
      "Epoch [11800/20000], Loss: 1.9479734874039423e-06\n",
      "Epoch [11900/20000], Loss: 1.4806049442039892e-10\n",
      "Epoch [12000/20000], Loss: 1.5702353696814275e-12\n",
      "Epoch [12100/20000], Loss: 1.8784281855671603e-12\n",
      "Epoch [12200/20000], Loss: 1.705853340527863e-12\n",
      "Epoch [12300/20000], Loss: 1.768571422644527e-08\n",
      "Epoch [12400/20000], Loss: 9.988703657606846e-13\n",
      "Epoch [12500/20000], Loss: 6.486742024601477e-13\n",
      "Epoch [12600/20000], Loss: 5.79228776487889e-13\n",
      "Epoch [12700/20000], Loss: 8.293125908220844e-11\n",
      "Epoch [12800/20000], Loss: 2.050449220136752e-11\n",
      "Epoch [12900/20000], Loss: 1.3895687985682192e-13\n",
      "Epoch [13000/20000], Loss: 1.565742300353376e-13\n",
      "Epoch [13100/20000], Loss: 8.245343252610837e-08\n",
      "Epoch [13200/20000], Loss: 1.409760004046634e-12\n",
      "Epoch [13300/20000], Loss: 9.035386913484877e-13\n",
      "Epoch [13400/20000], Loss: 8.907666813363913e-13\n",
      "Epoch [13500/20000], Loss: 9.014594626322037e-13\n",
      "Epoch [13600/20000], Loss: 9.011690048701948e-13\n",
      "Epoch [13700/20000], Loss: 9.013838395306728e-13\n",
      "Epoch [13800/20000], Loss: 2.8081001346436096e-07\n",
      "Epoch [13900/20000], Loss: 3.527428071858907e-10\n",
      "Epoch [14000/20000], Loss: 5.982888338397496e-13\n",
      "Epoch [14100/20000], Loss: 6.525288122338768e-13\n",
      "Epoch [14200/20000], Loss: 5.984760213448292e-13\n",
      "Epoch [14300/20000], Loss: 5.816171112435486e-13\n",
      "Epoch [14400/20000], Loss: 6.029567036630601e-13\n",
      "Epoch [14500/20000], Loss: 6.100456511476393e-13\n",
      "Epoch [14600/20000], Loss: 7.167930249352139e-08\n",
      "Epoch [14700/20000], Loss: 3.952661982442596e-12\n",
      "Epoch [14800/20000], Loss: 8.917261460489323e-13\n",
      "Epoch [14900/20000], Loss: 9.044939276725561e-13\n",
      "Epoch [15000/20000], Loss: 9.015725991289025e-13\n",
      "Epoch [15100/20000], Loss: 9.02956474781863e-13\n",
      "Epoch [15200/20000], Loss: 9.9172764184835e-13\n",
      "Epoch [15300/20000], Loss: 8.898292259279517e-13\n",
      "Epoch [15400/20000], Loss: 9.048593038046837e-13\n",
      "Epoch [15500/20000], Loss: 9.052909788996588e-13\n",
      "Epoch [15600/20000], Loss: 9.969569657666821e-13\n",
      "Epoch [15700/20000], Loss: 1.1637304453415709e-09\n",
      "Epoch [15800/20000], Loss: 8.202821560020224e-13\n",
      "Epoch [15900/20000], Loss: 9.587365352569516e-14\n",
      "Epoch [16000/20000], Loss: 1.7167848089319482e-13\n",
      "Epoch [16100/20000], Loss: 1.1911271394129258e-13\n",
      "Epoch [16200/20000], Loss: 1.1908781794890688e-13\n",
      "Epoch [16300/20000], Loss: 1.5756401175112678e-13\n",
      "Epoch [16400/20000], Loss: 8.476928996969946e-06\n",
      "Epoch [16500/20000], Loss: 1.0593491361898799e-11\n",
      "Epoch [16600/20000], Loss: 2.0955020487919973e-13\n",
      "Epoch [16700/20000], Loss: 2.0140134135079868e-13\n",
      "Epoch [16800/20000], Loss: 2.0144453325484507e-13\n",
      "Epoch [16900/20000], Loss: 2.0157886590401602e-13\n",
      "Epoch [17000/20000], Loss: 2.013961236278436e-13\n",
      "Epoch [17100/20000], Loss: 2.0139940333941536e-13\n",
      "Epoch [17200/20000], Loss: 6.053415972928633e-07\n",
      "Epoch [17300/20000], Loss: 3.685246899309824e-11\n",
      "Epoch [17400/20000], Loss: 2.0188200883144297e-13\n",
      "Epoch [17500/20000], Loss: 2.0161491562625117e-13\n",
      "Epoch [17600/20000], Loss: 2.0160588964316523e-13\n",
      "Epoch [17700/20000], Loss: 2.0158718715568985e-13\n",
      "Epoch [17800/20000], Loss: 2.0188142607277526e-13\n",
      "Epoch [17900/20000], Loss: 2.018566385006068e-13\n",
      "Epoch [18000/20000], Loss: 2.019737458877624e-13\n",
      "Epoch [18100/20000], Loss: 3.395760828084349e-08\n",
      "Epoch [18200/20000], Loss: 1.0086888464246047e-13\n",
      "Epoch [18300/20000], Loss: 9.688070793734474e-14\n",
      "Epoch [18400/20000], Loss: 9.587065841719367e-14\n",
      "Epoch [18500/20000], Loss: 9.59776962766723e-14\n",
      "Epoch [18600/20000], Loss: 1.1486982426454212e-13\n",
      "Epoch [18700/20000], Loss: 1.149945210669051e-13\n",
      "Epoch [18800/20000], Loss: 1.1897842737071396e-12\n",
      "Epoch [18900/20000], Loss: 1.5197451630477588e-10\n",
      "Epoch [19000/20000], Loss: 5.213082569788252e-13\n",
      "Epoch [19100/20000], Loss: 2.0617897309154615e-13\n",
      "Epoch [19200/20000], Loss: 2.3420936301477013e-08\n",
      "Epoch [19300/20000], Loss: 6.251557973548094e-13\n",
      "Epoch [19400/20000], Loss: 1.2108273639375305e-13\n",
      "Epoch [19500/20000], Loss: 4.330410519060024e-08\n",
      "Epoch [19600/20000], Loss: 4.02468900220776e-12\n",
      "Epoch [19700/20000], Loss: 9.59230321583883e-14\n",
      "Epoch [19800/20000], Loss: 9.238492810386204e-14\n",
      "Epoch [19900/20000], Loss: 9.23841691623413e-14\n",
      "Epoch [20000/20000], Loss: 2.559881289148497e-13\n",
      "Learned coefficients (PyTorch): [[ 1.2026571e-09  1.0000000e+00]\n",
      " [-3.9478416e+01 -1.2566371e+00]]\n",
      "LASSO coefficients: [[-8.71412794e-04  9.99349416e-01]\n",
      " [-3.94365977e+01 -1.25453641e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LassoCV, MultiTaskLassoCV\n",
    "\n",
    "# Generate synthetic data for a damped harmonic oscillator\n",
    "def generate_data(num_samples, damping_ratio, natural_frequency, dt):\n",
    "    t = np.arange(0, num_samples * dt, dt)\n",
    "    x = np.exp(-damping_ratio * natural_frequency * t) * np.cos(np.sqrt(1 - damping_ratio**2) * natural_frequency * t)\n",
    "    dxdt = -damping_ratio * natural_frequency * x - natural_frequency * np.exp(-damping_ratio * natural_frequency * t) * np.sin(np.sqrt(1 - damping_ratio**2) * natural_frequency * t)\n",
    "    return t, x, dxdt\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000\n",
    "damping_ratio = 0.1\n",
    "natural_frequency = 2 * np.pi\n",
    "dt = 0.01\n",
    "\n",
    "# Generate synthetic data\n",
    "t, x, dxdt = generate_data(num_samples, damping_ratio, natural_frequency, dt)\n",
    "\n",
    "# Construct library matrix\n",
    "Theta = np.vstack((x, dxdt)).T\n",
    "Xi = np.zeros((num_samples, 2))\n",
    "Xi[:, 0] = dxdt\n",
    "Xi[:, 1] = -2 * damping_ratio * natural_frequency * dxdt - natural_frequency**2 * x\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "Theta_tensor = torch.tensor(Theta, dtype=torch.float32)\n",
    "Xi_tensor = torch.tensor(Xi, dtype=torch.float32)\n",
    "\n",
    "# Define SINDy model\n",
    "class SINDy(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SINDy, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 2)  # Two terms: x_dot and -2*zeta*omega_n*x_dot - omega_n^2*x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Train the model\n",
    "model = SINDy(Theta.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 20000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(Theta_tensor)\n",
    "    loss = criterion(outputs, Xi_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Extract coefficients\n",
    "learned_coefficients = model.linear.weight.detach().numpy()\n",
    "\n",
    "# LASSO regularization for sparsity\n",
    "lasso = MultiTaskLassoCV()\n",
    "lasso.fit(Theta, Xi)\n",
    "lasso_coefficients = lasso.coef_\n",
    "\n",
    "print(\"Learned coefficients (PyTorch):\", learned_coefficients)\n",
    "print(\"LASSO coefficients:\", lasso_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.5530779e-07,  9.9999958e-01],\n",
       "       [-8.8166809e+00, -2.8912848e-01]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Maximum degree of the polynomial terms\u001b[39;00m\n\u001b[0;32m     54\u001b[0m sparsity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Sparsity parameter\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sindy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(coefficients)\n",
      "Cell \u001b[1;32mIn[28], line 30\u001b[0m, in \u001b[0;36mtrain_sindy\u001b[1;34m(X, y, degree, sparsity, lr, epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_sindy\u001b[39m(X, y, degree, sparsity, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     library_functions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_library_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m library_functions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[28], line 20\u001b[0m, in \u001b[0;36mgenerate_library_functions\u001b[1;34m(X, degree)\u001b[0m\n\u001b[0;32m     18\u001b[0m library_functions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m     21\u001b[0m         library_functions\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mpower(X[:, i], d))\n\u001b[0;32m     22\u001b[0m library_functions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(library_functions)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SINDy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, library_functions):\n",
    "        super(SINDy, self).__init__()\n",
    "        self.library_functions = library_functions\n",
    "        self.output_dim = output_dim\n",
    "        self.coefficients = nn.Parameter(torch.randn(len(library_functions), output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.coefficients)\n",
    "\n",
    "def generate_library_functions(X, degree):\n",
    "    # Generate library functions up to a certain degree\n",
    "    library_functions = []\n",
    "    for d in range(1, degree + 1):\n",
    "        for i in range(X.shape[1]):\n",
    "            library_functions.append(np.power(X[:, i], d))\n",
    "    library_functions = np.column_stack(library_functions)\n",
    "    return library_functions\n",
    "\n",
    "def lasso_loss(parameters, sparsity):\n",
    "    l1_penalty = torch.norm(parameters, 1)\n",
    "    return sparsity * l1_penalty\n",
    "\n",
    "def train_sindy(X, y, degree, sparsity, lr=0.01, epochs=100):\n",
    "    library_functions = generate_library_functions(X, degree)\n",
    "    input_dim = library_functions.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "\n",
    "    model = SINDy(input_dim, output_dim, library_functions)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.tensor(library_functions, dtype=torch.float32))\n",
    "        loss = torch.mean((output - torch.tensor(y, dtype=torch.float32)) ** 2) + lasso_loss(model.coefficients, sparsity)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    return model.coefficients.detach().numpy()\n",
    "\n",
    "# Example usage\n",
    "# Assuming X is your input data (features) and y is your target data\n",
    "X = np.array(x_train)\n",
    "y = np.array(x_train)\n",
    "\n",
    "degree = 2  # Maximum degree of the polynomial terms\n",
    "sparsity = 0.1  # Sparsity parameter\n",
    "coefficients = train_sindy(X, y, degree, sparsity)\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SINDy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sparsity):\n",
    "        super(SINDy, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sparse_loss = nn.L1Loss()\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def lasso_loss(self, parameters):\n",
    "        l1_penalty = torch.norm(parameters, 1)\n",
    "        return self.sparsity * l1_penalty\n",
    "    \n",
    "    def fit(self, X, y, lr=0.01, epochs=100):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(X)\n",
    "            loss = self.sparse_loss(output, y) + self.lasso_loss(self.linear.weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Example usage\n",
    "# Assuming X is your input data (features) and y is your target data\n",
    "X = torch.tensor(x_train, dtype=torch.float32)\n",
    "y = torch.tensor(x_train, dtype=torch.float32)\n",
    "\n",
    "input_dim = X.shape[1]  # Number of input features\n",
    "output_dim = y.shape[1]  # Number of output features\n",
    "sparsity = 0.1  # Sparsity parameter\n",
    "\n",
    "model = SINDy(input_dim, output_dim, sparsity)\n",
    "model.fit(X, y)\n",
    "\n",
    "coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LassoRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, alpha):\n",
    "        super(LassoRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def l1_penalty(self):\n",
    "        l1_crit = torch.nn.L1Loss(reduction='sum')\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += l1_crit(param, torch.zeros_like(param))\n",
    "        return self.alpha * reg_loss\n",
    "\n",
    "# Sample data\n",
    "X = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "Y = torch.randn(100, 1)    # 100 samples, 1 output\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "model = LassoRegression(input_size=10, output_size=1, alpha=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    # Loss\n",
    "    loss = criterion(outputs, Y) + model.l1_penalty()\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# After training\n",
    "print('Final Lasso regression coefficients:')\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(name, param.data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
